{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#make sure to be in microsoft store python\n",
    "\n",
    "rotten_tomatoes_reviews = pd.read_csv('rotten_tomatoes_critic_reviews.csv')\n",
    "semanlysis_dataset = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89576\n",
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "# Assuming 'semanlysis_dataset' is your dataframe\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(semanlysis_dataset, test_size=0.25, train_size=.75, random_state=42)\n",
    "\n",
    "# Define features and labels\n",
    "train_x, train_y = train['review'], train['sentiment']\n",
    "test_x, test_y = test['review'], test['sentiment']\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "train_x_vector = tfidf.fit_transform(train_x)\n",
    "test_x_vector = tfidf.transform(test_x)\n",
    "\n",
    "with open ('model_tfidf', 'wb') as files:\n",
    "    pickle.dump(tfidf, files)\n",
    "# Define and train the model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_x_vector, train_y)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = log_reg.score(test_x_vector, test_y)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "prediction = log_reg.predict(tfidf.transform(['I did not like this movie at all']))\n",
    "print(prediction)\n",
    "\n",
    "\n",
    "with open ('model_log_reg', 'wb') as files:\n",
    "    pickle.dump(log_reg, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review sentiment\n",
      "Id                                                             \n",
      "1   One of the other reviewers has mentioned that ...  positive\n",
      "2   A wonderful little production. <br /><br />The...  positive\n",
      "3   I thought this was a wonderful way to spend ti...  positive\n",
      "4   Basically there's a family where a little boy ...  negative\n",
      "5   Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "print(semanlysis_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  review_score                                     review_content numerator  \\\n",
      "3        3.5/5  Whether audiences will get behind The Lightnin...       3.5   \n",
      "6          1/4  Harry Potter knockoffs don't come more transpa...         1   \n",
      "7        3.5/5  Percy Jackson isn't a great movie, but it's a ...       3.5   \n",
      "8            B                         Fun, brisk and imaginative         B   \n",
      "9          3/5  Crammed with dragons, set-destroying fights an...         3   \n",
      "\n",
      "   denominator  bool_denominator  _4_column  _5_column  _10_column  \n",
      "3            5             False      False       True       False  \n",
      "6            4             False       True      False       False  \n",
      "7            5             False      False       True       False  \n",
      "8            0              True      False      False       False  \n",
      "9            5             False      False       True       False  \n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['rotten_tomatoes_link', 'critic_name','top_critic', 'publisher_name', 'review_type', 'review_date' ]\n",
    "rotten_tomatoes_reviews = rotten_tomatoes_reviews.drop(columns=columns_to_drop)\n",
    "rotten_tomatoes_reviews = rotten_tomatoes_reviews[~rotten_tomatoes_reviews['review_score'].isnull()]\n",
    "rotten_tomatoes_reviews = rotten_tomatoes_reviews.dropna()\n",
    "\n",
    "#Convert letters to values \n",
    "#and make defined scale and converter\n",
    "\n",
    "print(rotten_tomatoes_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  review_score                                     review_content numerator  \\\n",
      "3        3.5/5  Whether audiences will get behind The Lightnin...       3.5   \n",
      "6          1/4  Harry Potter knockoffs don't come more transpa...         1   \n",
      "7        3.5/5  Percy Jackson isn't a great movie, but it's a ...       3.5   \n",
      "8            B                         Fun, brisk and imaginative         B   \n",
      "9          3/5  Crammed with dragons, set-destroying fights an...         3   \n",
      "\n",
      "  denominator  bool_denominator  _4_column  _5_column  _10_column  \n",
      "3           5             False      False       True       False  \n",
      "6           4             False       True      False       False  \n",
      "7           5             False      False       True       False  \n",
      "8        None              True      False      False       False  \n",
      "9           5             False      False       True       False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "rotten_tomatoes_reviews[['numerator', 'denominator']] = rotten_tomatoes_reviews['review_score'].str.split('/', expand=True)\n",
    "\n",
    "print(rotten_tomatoes_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        review_score                                     review_content  \\\n",
      "3              3.5/5  Whether audiences will get behind The Lightnin...   \n",
      "6                1/4  Harry Potter knockoffs don't come more transpa...   \n",
      "7              3.5/5  Percy Jackson isn't a great movie, but it's a ...   \n",
      "8                  B                         Fun, brisk and imaginative   \n",
      "9                3/5  Crammed with dragons, set-destroying fights an...   \n",
      "...              ...                                                ...   \n",
      "1130004          5/5                         The movie is a revelation.   \n",
      "1130013        3.5/5  Seen today, it's not only a startling indictme...   \n",
      "1130014           B+  A rousing visual spectacle that's a prequel of...   \n",
      "1130015        3.5/5  A simple two-act story: Prelude to war, and th...   \n",
      "1130016            C  Rides the line between being a pure artifact o...   \n",
      "\n",
      "        numerator  denominator  bool_denominator  _4_column  _5_column  \\\n",
      "3             3.5            5             False      False       True   \n",
      "6               1            4             False       True      False   \n",
      "7             3.5            5             False      False       True   \n",
      "8               B            0              True      False      False   \n",
      "9               3            5             False      False       True   \n",
      "...           ...          ...               ...        ...        ...   \n",
      "1130004         5            5             False      False       True   \n",
      "1130013       3.5            5             False      False       True   \n",
      "1130014        B+            0              True      False      False   \n",
      "1130015       3.5            5             False      False       True   \n",
      "1130016         C            0              True      False      False   \n",
      "\n",
      "         _10_column  \n",
      "3             False  \n",
      "6             False  \n",
      "7             False  \n",
      "8             False  \n",
      "9             False  \n",
      "...             ...  \n",
      "1130004       False  \n",
      "1130013       False  \n",
      "1130014       False  \n",
      "1130015       False  \n",
      "1130016       False  \n",
      "\n",
      "[752734 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "null_column = rotten_tomatoes_reviews['denominator'].isnull()\n",
    "rotten_tomatoes_reviews['bool_denominator'] = null_column\n",
    "rotten_tomatoes_reviews['denominator'] = rotten_tomatoes_reviews['denominator'].fillna(0)\n",
    "rotten_tomatoes_reviews['denominator'] = pd.to_numeric(rotten_tomatoes_reviews['denominator'], errors='coerce')\n",
    "\n",
    "rotten_tomatoes_reviews['denominator'] = rotten_tomatoes_reviews['denominator'].astype(int)\n",
    "rotten_tomatoes_reviews['_4_column'] = np.where(rotten_tomatoes_reviews['denominator'] == 4, True, False)\n",
    "rotten_tomatoes_reviews['_5_column'] = np.where(rotten_tomatoes_reviews['denominator'] == 5, True, False)\n",
    "rotten_tomatoes_reviews['_10_column'] = np.where(rotten_tomatoes_reviews['denominator'] == 10, True, False)\n",
    "rotten_tomatoes_reviews['_4_column'].fillna(False, inplace=True)\n",
    "rotten_tomatoes_reviews['_5_column'].fillna(False, inplace=True)\n",
    "rotten_tomatoes_reviews['_10_column'].fillna(False, inplace=True)\n",
    "print(rotten_tomatoes_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_score                                     review_content numerator  \\\n",
      "3         3.5/5  Whether audiences will get behind The Lightnin...       3.5   \n",
      "6           1/4  Harry Potter knockoffs don't come more transpa...         1   \n",
      "7         3.5/5  Percy Jackson isn't a great movie, but it's a ...       3.5   \n",
      "9           3/5  Crammed with dragons, set-destroying fights an...         3   \n",
      "10          4/5  This action-packed fantasy adventure, based on...         4   \n",
      "\n",
      "    denominator  bool_denominator  _4_column  _5_column  _10_column  \\\n",
      "3             5             False      False       True       False   \n",
      "6             4             False       True      False       False   \n",
      "7             5             False      False       True       False   \n",
      "9             5             False      False       True       False   \n",
      "10            5             False      False       True       False   \n",
      "\n",
      "   review_score_adjusted  \n",
      "3                  12/20  \n",
      "6                   5/20  \n",
      "7                  12/20  \n",
      "9                  12/20  \n",
      "10                 16/20  \n"
     ]
    }
   ],
   "source": [
    "#common denomiator = to 20\n",
    "def convert_denominator(row):\n",
    "    adjusted_score = None  # Define a default value\n",
    "    if row['_4_column'] == True or row['_5_column'] == True or row['_10_column'] == True:\n",
    "        row['numerator'] = pd.to_numeric(row['numerator'], errors='coerce').astype(int)\n",
    "\n",
    "    if row['_4_column'] == True:\n",
    "        score = 5*row['numerator']\n",
    "        adjusted_score = f\"{score}/20\"\n",
    "\n",
    "    elif row['_5_column'] == True: \n",
    "        score = 4*row['numerator']\n",
    "        adjusted_score = f\"{score}/20\"\n",
    "\n",
    "    elif row['_10_column'] == True:\n",
    "        score = 2*row['numerator']\n",
    "        adjusted_score = f\"{score}/20\"\n",
    "\n",
    "    \n",
    "    return adjusted_score\n",
    "\n",
    "# Assuming 'bool_denominator' is already defined in your DataFrame\n",
    "rotten_tomatoes_reviews['review_score_adjusted'] = rotten_tomatoes_reviews.apply(convert_denominator, axis=1)\n",
    "rotten_tomatoes_reviews = rotten_tomatoes_reviews.drop(rotten_tomatoes_reviews[rotten_tomatoes_reviews['bool_denominator'] == True].index)\n",
    "print(rotten_tomatoes_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   original_review_score                                     review_content  \\\n",
      "3                  3.5/5  Whether audiences will get behind The Lightnin...   \n",
      "6                    1/4  Harry Potter knockoffs don't come more transpa...   \n",
      "7                  3.5/5  Percy Jackson isn't a great movie, but it's a ...   \n",
      "9                    3/5  Crammed with dragons, set-destroying fights an...   \n",
      "10                   4/5  This action-packed fantasy adventure, based on...   \n",
      "\n",
      "   review_score_adjusted  \n",
      "3                  12/20  \n",
      "6                   5/20  \n",
      "7                  12/20  \n",
      "9                  12/20  \n",
      "10                 16/20  \n"
     ]
    }
   ],
   "source": [
    "# Delete unused columns\n",
    "\n",
    "columns_to_delete = ['numerator','denominator','bool_denominator', '_4_column', '_5_column','_10_column']\n",
    "rotten_tomatoes_reviews.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "rotten_tomatoes_reviews.rename(columns={'review_score': 'original_review_score'}, inplace=True)\n",
    "#rotten_tomatoes_reviews[['numerator', 'denominator']] = rotten_tomatoes_reviews['review_score_adjusted'].str.split('/', expand=True)\n",
    "#rotten_tomatoes_reviews['numerator'] = rotten_tomatoes_reviews['numerator'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "rotten_tomatoes_reviews.to_csv('Rotten_Tomato_Adjusted_Data.csv', index=False)\n",
    "\n",
    "\n",
    "print(rotten_tomatoes_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629257\n",
      "629257\n",
      "  original_review_score                                     review_content  \\\n",
      "0                 3.5/5  Whether audiences will get behind The Lightnin...   \n",
      "1                   1/4  Harry Potter knockoffs don't come more transpa...   \n",
      "2                 3.5/5  Percy Jackson isn't a great movie, but it's a ...   \n",
      "3                   3/5  Crammed with dragons, set-destroying fights an...   \n",
      "4                   4/5  This action-packed fantasy adventure, based on...   \n",
      "\n",
      "  review_score_adjusted  \n",
      "0                 12/20  \n",
      "1                  5/20  \n",
      "2                 12/20  \n",
      "3                 12/20  \n",
      "4                 16/20  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "rotten_tomatoes_reviews_adjusted = pd.read_csv('Rotten_Tomato_Adjusted_Data.csv')\n",
    "print(len(rotten_tomatoes_reviews_adjusted.index))\n",
    "rotten_tomatoes_reviews_adjusted.dropna()\n",
    "print(len(rotten_tomatoes_reviews_adjusted.index))\n",
    "\n",
    "\n",
    "print(rotten_tomatoes_reviews_adjusted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mmorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mmorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>preprocess_txt</th>\n",
       "      <th>total_len</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>[whether, audience, get, behind, lightning, th...</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter knockoffs don't come more transpa...</td>\n",
       "      <td>[harry, potter, knockoff, come, transparent, s...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Percy Jackson isn't a great movie, but it's a ...</td>\n",
       "      <td>[percy, jackson, great, movie, good, one, trot...</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crammed with dragons, set-destroying fights an...</td>\n",
       "      <td>[crammed, dragon, set, destroying, fight, thin...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This action-packed fantasy adventure, based on...</td>\n",
       "      <td>[action, packed, fantasy, adventure, based, ri...</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content  \\\n",
       "0  Whether audiences will get behind The Lightnin...   \n",
       "1  Harry Potter knockoffs don't come more transpa...   \n",
       "2  Percy Jackson isn't a great movie, but it's a ...   \n",
       "3  Crammed with dragons, set-destroying fights an...   \n",
       "4  This action-packed fantasy adventure, based on...   \n",
       "\n",
       "                                      preprocess_txt  total_len  pos_count  \\\n",
       "0  [whether, audience, get, behind, lightning, th...         20          3   \n",
       "1  [harry, potter, knockoff, come, transparent, s...         12          1   \n",
       "2  [percy, jackson, great, movie, good, one, trot...         15          3   \n",
       "3  [crammed, dragon, set, destroying, fight, thin...         21          1   \n",
       "4  [action, packed, fantasy, adventure, based, ri...         23          3   \n",
       "\n",
       "   neg_count  sentiment  \n",
       "0          2       0.05  \n",
       "1          0       0.08  \n",
       "2          0       0.20  \n",
       "3          1       0.00  \n",
       "4          1       0.09  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "df = pd.read_csv('Rotten_Tomato_Adjusted_Data.csv', usecols=['review_content'])\n",
    "lemma = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "def text_prep(x: str) -> list:\n",
    "     corp = str(x).lower() \n",
    "     corp = re.sub('[^a-zA-Z]+',' ', corp).strip() \n",
    "     tokens = word_tokenize(corp)\n",
    "     words = [t for t in tokens if t not in stop_words]\n",
    "     lemmatize = [lemma.lemmatize(w) for w in words]\n",
    "     return lemmatize\n",
    "preprocess_tag = [text_prep(i) for i in df['review_content']]\n",
    "df[\"preprocess_txt\"] = preprocess_tag\n",
    "df['total_len'] = df['preprocess_txt'].map(lambda x: len(x))\n",
    "file = open('negative_word_list.txt', 'r')\n",
    "neg_words = file.read().split()\n",
    "file = open('positive_word_list.txt', 'r')\n",
    "pos_words = file.read().split()\n",
    "num_pos = df['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))\n",
    "df['pos_count'] = num_pos\n",
    "num_neg = df['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))\n",
    "df['neg_count'] = num_neg\n",
    "df['sentiment'] = round((df['pos_count'] - df['neg_count']) / df['total_len'], 2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      review_content  \\\n",
      "0  Whether audiences will get behind The Lightnin...   \n",
      "1  Harry Potter knockoffs don't come more transpa...   \n",
      "2  Percy Jackson isn't a great movie, but it's a ...   \n",
      "3  Crammed with dragons, set-destroying fights an...   \n",
      "4  This action-packed fantasy adventure, based on...   \n",
      "\n",
      "                                      preprocess_txt  total_len  pos_count  \\\n",
      "0  [whether, audience, get, behind, lightning, th...         20          3   \n",
      "1  [harry, potter, knockoff, come, transparent, s...         12          1   \n",
      "2  [percy, jackson, great, movie, good, one, trot...         15          3   \n",
      "3  [crammed, dragon, set, destroying, fight, thin...         21          1   \n",
      "4  [action, packed, fantasy, adventure, based, ri...         23          3   \n",
      "\n",
      "   neg_count  sentiment review_score  predicted_sentiment  \n",
      "0          2       0.05           12                  1.0  \n",
      "1          0       0.08            5                  0.0  \n",
      "2          0       0.20           12                  1.0  \n",
      "3          1       0.00           12                  1.0  \n",
      "4          1       0.09           16                  1.0  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "reviews = pd.read_csv('Rotten_Tomato_Adjusted_Data.csv')\n",
    "reviews[['numerator', 'denominator']] = reviews['review_score_adjusted'].str.split('/', expand=True)\n",
    "\n",
    "extracted_cl = reviews['numerator']\n",
    "\n",
    "\n",
    "df2 = pd.concat([df, extracted_cl.rename(\"review_score\")], axis=1)\n",
    "\n",
    "# Load the saved models\n",
    "with open('model_log_reg', 'rb') as f:\n",
    "    log_reg = pickle.load(f)\n",
    "\n",
    "with open('model_tfidf', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Read data from the file\n",
    "\n",
    "# Assuming your DataFrame has a column 'review' containing text data\n",
    "text_data = df2['review_content']\n",
    "\n",
    "# TF-IDF vectorization\n",
    "text_data_vectorized = tfidf.transform(text_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = log_reg.predict(text_data_vectorized)\n",
    "\n",
    "# Create a new column with the predicted sentiments\n",
    "df2['predicted_sentiment'] = predictions\n",
    "\n",
    "df2['predicted_sentiment'] = df2['predicted_sentiment'].map({'positive': 1.0, 'negative': 0.0})\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(df2.head())\n",
    "df2.dropna()\n",
    "\n",
    "df2.to_csv('Rotten_Tomato_Sentiment.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 322. GiB for an array with shape (466285, 92545) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmorr\\OneDrive\\Documents\\GitHub\\Movie-Review-Estimater\\movie_reviews.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m test_x_text_vector \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mtransform(test[\u001b[39m'\u001b[39m\u001b[39mreview_content\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X15sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Concatenate the text vectorization with the other features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m train_x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([pd\u001b[39m.\u001b[39mDataFrame(train_x_text_vector\u001b[39m.\u001b[39;49mtoarray()), train_x\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X15sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m test_x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([pd\u001b[39m.\u001b[39mDataFrame(test_x_text_vector\u001b[39m.\u001b[39mtoarray()), test_x\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Define and train the model (using Linear Regression for regression task)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 322. GiB for an array with shape (466285, 92545) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def analyze_sentiment(comment: str) -> pd.DataFrame:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def text_prep(x: str) -> list:\n",
    "        corp = str(x).lower() \n",
    "        corp = re.sub('[^a-zA-Z]+',' ', corp).strip() \n",
    "        tokens = word_tokenize(corp)\n",
    "        words = [t for t in tokens if t not in stop_words]\n",
    "        lemmatize = [lemma.lemmatize(w) for w in words]\n",
    "        return lemmatize\n",
    "\n",
    "    # Create a DataFrame with the provided comment\n",
    "    df = pd.DataFrame({'review_content': [comment]})\n",
    "\n",
    "    # Apply text preprocessing to the comment\n",
    "    preprocess_tag = [text_prep(i) for i in df['review_content']]\n",
    "    df[\"preprocess_txt\"] = preprocess_tag\n",
    "    df['total_len'] = df['preprocess_txt'].map(lambda x: len(x))\n",
    "\n",
    "    # Load positive and negative word lists\n",
    "    file = open('negative_word_list.txt', 'r')\n",
    "    neg_words = file.read().split()\n",
    "    file = open('positive_word_list.txt', 'r')\n",
    "    pos_words = file.read().split()\n",
    "\n",
    "    # Count positive and negative words in the comment\n",
    "    num_pos = df['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))\n",
    "    df['pos_count'] = num_pos\n",
    "    num_neg = df['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))\n",
    "    df['neg_count'] = num_neg\n",
    "\n",
    "    # Compute sentiment score\n",
    "    df['sentiment'] = round((df['pos_count'] - df['neg_count']) / df['total_len'], 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "rotten_tomatoes_reviews_adjusted = pd.read_csv('Rotten_Tomato_Sentiment.csv')\n",
    "\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "rotten_tomatoes_reviews_adjusted = rotten_tomatoes_reviews_adjusted.dropna(subset=['sentiment', 'neg_count', 'pos_count', 'total_len', 'review_content'])\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(rotten_tomatoes_reviews_adjusted, test_size=0.25, train_size=0.75, random_state=42)\n",
    "\n",
    "# Fill NaN values in 'review_content' with an empty string\n",
    "train['review_content'].fillna('', inplace=True)\n",
    "test['review_content'].fillna('', inplace=True)\n",
    "\n",
    "# Define features and labels\n",
    "feature_cols = ['sentiment', 'neg_count', 'pos_count']\n",
    "train_x, train_y = train[feature_cols], train['review_content']\n",
    "test_x, test_y = test[feature_cols], test['review_content']\n",
    "\n",
    "# TF-IDF vectorization for the 'review_content' column\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "train_x_text_vector = tfidf.fit_transform(train['review_content'])\n",
    "test_x_text_vector = tfidf.transform(test['review_content'])\n",
    "\n",
    "# Concatenate the text vectorization with the other features\n",
    "train_x = pd.concat([pd.DataFrame(train_x_text_vector.toarray()), train_x.reset_index(drop=True)], axis=1)\n",
    "test_x = pd.concat([pd.DataFrame(test_x_text_vector.toarray()), test_x.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define and train the model (using Linear Regression for regression task)\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = linear_reg.predict(test_x)\n",
    "mse = mean_squared_error(test_y, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Make predictions for a sample review\n",
    "sample_review = 'I did not like this movie at all'\n",
    "sample_sentiment = analyze_sentiment(sample_review)['sentiment'][0]\n",
    "sample_review_vector = tfidf.transform([sample_review])\n",
    "sample_review_features = pd.DataFrame([[sample_sentiment, 1, 0, len(sample_review)]], columns=feature_cols)\n",
    "sample_review_features_vector = pd.concat([pd.DataFrame(sample_review_vector.toarray()), sample_review_features.reset_index(drop=True)], axis=1)\n",
    "prediction = linear_reg.predict(sample_review_features_vector)\n",
    "print(f\"Predicted Score: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sentimentneg_count'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmorr\\OneDrive\\Documents\\GitHub\\Movie-Review-Estimater\\movie_reviews.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X16sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m train_batch_text_vector \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(train_batch[\u001b[39m'\u001b[39m\u001b[39mreview_content\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X16sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m train_batch_x_text \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(train_batch_text_vector\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39m[\u001b[39mstr\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_features)])  \u001b[39m# Convert to DataFrame with string column names\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X16sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m train_batch_x_other \u001b[39m=\u001b[39m train_batch[feature_cols]\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X16sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m train_batch_x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([train_batch_x_text, train_batch_x_other], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X16sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m train_batch_y \u001b[39m=\u001b[39m train_batch[\u001b[39m'\u001b[39m\u001b[39mreview_score\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# Define train_batch_y\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['sentimentneg_count'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "from math import floor\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(comment: str) -> pd.DataFrame:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def text_prep(x: str) -> list:\n",
    "        corp = str(x).lower() \n",
    "        corp = re.sub('[^a-zA-Z]+',' ', corp).strip() \n",
    "        tokens = word_tokenize(corp)\n",
    "        words = [t for t in tokens if t not in stop_words]\n",
    "        lemmatize = [lemma.lemmatize(w) for w in words]\n",
    "        return lemmatize\n",
    "\n",
    "    # Create a DataFrame with the provided comment\n",
    "    df = pd.DataFrame({'review_content': [comment]})\n",
    "\n",
    "    # Apply text preprocessing to the comment\n",
    "    preprocess_tag = [text_prep(i) for i in df['review_content']]\n",
    "    df[\"preprocess_txt\"] = preprocess_tag\n",
    "    df['total_len'] = df['preprocess_txt'].map(lambda x: len(x))\n",
    "\n",
    "    # Load positive and negative word lists\n",
    "    file = open('negative_word_list.txt', 'r')\n",
    "    neg_words = file.read().split()\n",
    "    file = open('positive_word_list.txt', 'r')\n",
    "    pos_words = file.read().split()\n",
    "\n",
    "    # Count positive and negative words in the comment\n",
    "    num_pos = df['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))\n",
    "    df['pos_count'] = num_pos\n",
    "    num_neg = df['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))\n",
    "    df['neg_count'] = num_neg\n",
    "\n",
    "    # Compute sentiment score\n",
    "    df['sentiment'] = round((df['pos_count'] - df['neg_count']) / df['total_len'], 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "rotten_tomatoes_reviews_adjusted = pd.read_csv('Rotten_Tomato_Sentiment.csv')\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "rotten_tomatoes_reviews_adjusted = rotten_tomatoes_reviews_adjusted.dropna(subset=['sentiment', 'neg_count', 'pos_count', 'total_len', 'review_content', 'review_score'])\n",
    "\n",
    "# Ensure 'review_score' is treated as a numerical column\n",
    "rotten_tomatoes_reviews_adjusted['review_score'] = pd.to_numeric(rotten_tomatoes_reviews_adjusted['review_score'], errors='coerce')\n",
    "rotten_tomatoes_reviews_adjusted = rotten_tomatoes_reviews_adjusted.dropna(subset=['review_score'])\n",
    "\n",
    "# Fill NaN values in 'review_content' with an empty string\n",
    "rotten_tomatoes_reviews_adjusted['review_content'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(rotten_tomatoes_reviews_adjusted, test_size=0.25, train_size=0.75, random_state=42)\n",
    "\n",
    "# Define features and labels\n",
    "feature_cols = ['sentiment' 'neg_count', 'pos_count']\n",
    "train_y = train['review_score']\n",
    "test_y = test['review_score']\n",
    "\n",
    "# TF-IDF vectorization for the 'review_content' column\n",
    "max_features = 500  # Adjust as needed\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=max_features)\n",
    "\n",
    "# Split the training data into batches (adjust the batch size as needed)\n",
    "batch_size = 500\n",
    "num_batches = len(train) // batch_size\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "previous_mse = float('inf')\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "    train_batch = train.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Vectorize the current batch\n",
    "    train_batch_text_vector = tfidf.fit_transform(train_batch['review_content'])\n",
    "    train_batch_x_text = pd.DataFrame(train_batch_text_vector.toarray(), columns=[str(x) for x in range(max_features)])  # Convert to DataFrame with string column names\n",
    "    train_batch_x_other = train_batch[feature_cols].reset_index(drop=True)\n",
    "    train_batch_x = pd.concat([train_batch_x_text, train_batch_x_other], axis=1)\n",
    "    train_batch_y = train_batch['review_score']  # Define train_batch_y\n",
    "\n",
    "    # Train the model on the current batch\n",
    "    linear_reg.fit(train_batch_x, train_batch_y)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_x_text_vector = tfidf.transform(test['review_content'])\n",
    "    test_x_text = pd.DataFrame(test_x_text_vector.toarray(), columns=[str(x) for x in range(max_features)])  # Convert to DataFrame with string column names\n",
    "    test_x_other = test[feature_cols].reset_index(drop=True)\n",
    "    test_x = pd.concat([test_x_text, test_x_other], axis=1)\n",
    "    predictions = linear_reg.predict(test_x)\n",
    "    mse = mean_squared_error(test_y, predictions)\n",
    "    print(f\"Mean Squared Error (Batch {i + 1}/{num_batches}): {mse}\")\n",
    "    mse = floor(mse) \n",
    "    if mse < 300:\n",
    "        print(f\"Stopping training as Mean Squared Error is less than 300: {mse}\")\n",
    "        break\n",
    "\n",
    "    # Check if mean squared error is not decreasing significantly\n",
    "    \n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(linear_reg, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- review_length\n- sentiment\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mmorr\\OneDrive\\Documents\\GitHub\\Movie-Review-Estimater\\movie_reviews.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m sample_review_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([sample_review_features_text, sample_review_features_other], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Make predictions using the loaded model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m prediction \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39;49mpredict(sample_review_features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mmorr/OneDrive/Documents/GitHub/Movie-Review-Estimater/movie_reviews.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicted Score: \u001b[39m\u001b[39m{\u001b[39;00mprediction[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    517\u001b[0m ):\n\u001b[0;32m    518\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    583\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    503\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    504\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[1;32m--> 507\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- review_length\n- sentiment\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# Make predictions for a sample review\n",
    "sample_review = 'I did not like this movie at all'\n",
    "sample_sentiment = analyze_sentiment(sample_review)['sentiment'][0]\n",
    "sample_review_vector = tfidf.transform([sample_review])\n",
    "sample_review_features_text = pd.DataFrame(sample_review_vector.toarray(), columns=[str(x) for x in range(max_features)])\n",
    "\n",
    "# Define additional features for the sample review\n",
    "sample_review_features_other = pd.DataFrame([[sample_sentiment, 0, 1, len(sample_review)]], columns=['neg_count', 'pos_count', 'sentiment', 'review_length'])\n",
    "\n",
    "# Reorder the columns to match the order during training\n",
    "sample_review_features_other = sample_review_features_other[['neg_count', 'pos_count', 'sentiment', 'review_length']]\n",
    "\n",
    "# Concatenate the dataframes\n",
    "sample_review_features = pd.concat([sample_review_features_text, sample_review_features_other], axis=1)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "prediction = loaded_model.predict(sample_review_features)\n",
    "print(f\"Predicted Score: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 15.678193400070688\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(comment: str) -> pd.DataFrame:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def text_prep(x: str) -> list:\n",
    "        corp = str(x).lower()\n",
    "        corp = re.sub('[^a-zA-Z]+', ' ', corp).strip()\n",
    "        tokens = word_tokenize(corp)\n",
    "        words = [t for t in tokens if t not in stop_words]\n",
    "        lemmatize = [lemma.lemmatize(w) for w in words]\n",
    "        return lemmatize\n",
    "\n",
    "    # Create a DataFrame with the provided comment\n",
    "    df = pd.DataFrame({'review_content': [comment]})\n",
    "\n",
    "    # Apply text preprocessing to the comment\n",
    "    preprocess_tag = [text_prep(i) for i in df['review_content']]\n",
    "    df[\"preprocess_txt\"] = preprocess_tag\n",
    "    df['total_len'] = df['preprocess_txt'].map(lambda x: len(x))\n",
    "\n",
    "    # Load positive and negative word lists\n",
    "    with open('negative_word_list.txt', 'r') as file:\n",
    "        neg_words = file.read().split()\n",
    "    with open('positive_word_list.txt', 'r') as file:\n",
    "        pos_words = file.read().split()\n",
    "\n",
    "    # Count positive and negative words in the comment\n",
    "    num_pos = df['preprocess_txt'].map(lambda x: len([i for i in x if i in pos_words]))\n",
    "    df['pos_count'] = num_pos\n",
    "    num_neg = df['preprocess_txt'].map(lambda x: len([i for i in x if i in neg_words]))\n",
    "    df['neg_count'] = num_neg\n",
    "\n",
    "    # Compute sentiment score\n",
    "    df['sentiment'] = round((df['pos_count'] - df['neg_count']) / df['total_len'], 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "rotten_tomatoes_reviews_adjusted = pd.read_csv('Rotten_Tomato_Sentiment.csv')\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "rotten_tomatoes_reviews_adjusted = rotten_tomatoes_reviews_adjusted.dropna(subset=['sentiment', 'neg_count', 'pos_count', 'total_len', 'predicted_sentiment', 'review_score'])\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(rotten_tomatoes_reviews_adjusted, test_size=0.25, train_size=0.75, random_state=42)\n",
    "\n",
    "# Define features and labels\n",
    "feature_cols = ['sentiment', 'neg_count', 'pos_count', 'total_len', 'predicted_sentiment']\n",
    "train_y = train['review_score']\n",
    "test_y = test['review_score']\n",
    "\n",
    "# Define additional features\n",
    "train_x_other = train[feature_cols].reset_index(drop=True)\n",
    "test_x_other = test[feature_cols].reset_index(drop=True)\n",
    "\n",
    "# Define and train the model (using Linear Regression for regression task)\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(train_x_other, train_y)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = linear_reg.predict(test_x_other)\n",
    "mse = mean_squared_error(test_y, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "with open ('model_sentiment_values', 'wb') as files:\n",
    "        pickle.dump(linear_reg, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for a sample review\n",
    "with open('model_sentiment_values' , 'rb') as f:\n",
    "    linear_reg = pickle.load(f)\n",
    "sample_review = 'I did not like this movie at all'\n",
    "sample_df = analyze_sentiment(sample_review)[feature_cols]\n",
    "prediction = linear_reg.predict(sample_df)\n",
    "print(f\"Predicted Score: {prediction[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
